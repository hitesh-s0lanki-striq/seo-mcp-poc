{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174e73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    Thin wrapper over ChatOpenAI so we can swap models later easily.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-4.1\", temperature: float = 0.2):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "    async def call(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_content: str,\n",
    "        extra_messages: List[Dict[str, str]] | None = None,\n",
    "        tools: List[Any] | None = None,\n",
    "    ) -> AIMessage:\n",
    "        messages = [SystemMessage(content=system_prompt)]\n",
    "\n",
    "        if extra_messages:\n",
    "            for msg in extra_messages:\n",
    "                role = msg[\"role\"]\n",
    "                content = msg[\"content\"]\n",
    "                if role == \"user\":\n",
    "                    messages.append(HumanMessage(content=content))\n",
    "                elif role == \"assistant\":\n",
    "                    messages.append(AIMessage(content=content))\n",
    "\n",
    "        messages.append(HumanMessage(content=user_content))\n",
    "\n",
    "        if tools:\n",
    "            # Tool-enabled call\n",
    "            chain = self.llm.bind_tools(tools)\n",
    "            return await chain.ainvoke(messages)\n",
    "        else:\n",
    "            return await self.llm.ainvoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc045ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal, TypedDict\n",
    "\n",
    "RouteType = Literal[\"seo\", \"other\", \"mixed\"]\n",
    "\n",
    "class RouterOutput(TypedDict):\n",
    "    route: RouteType\n",
    "    confidence: float\n",
    "\n",
    "ROUTER_SYSTEM_PROMPT = \"\"\"\n",
    "        You are a routing classifier.\n",
    "        Goal: Decide if the user query is about SEO or something else.\n",
    "\n",
    "        Valid routes:\n",
    "        - \"seo\"   -> Search Engine Optimization, keywords, traffic, SERP, GSC, DataForSEO, etc.\n",
    "        - \"other\" -> Anything else not SEO-related.\n",
    "        - \"mixed\" -> Contains both SEO and non-SEO aspects.\n",
    "\n",
    "        Return STRICT JSON:\n",
    "        {\"route\": \"seo\" | \"other\" | \"mixed\", \"confidence\": 0.0-1.0}\n",
    "    \"\"\"\n",
    "\n",
    "class RouterAgent:\n",
    "    def __init__(self, llm: LLMClient):\n",
    "        self.llm = llm\n",
    "\n",
    "    async def route(self, user_query: str) -> RouterOutput:\n",
    "        resp = await self.llm.call(ROUTER_SYSTEM_PROMPT, user_query)\n",
    "        # LLM is instructed to return JSON; parse defensively\n",
    "        try:\n",
    "            data = json.loads(resp.content)\n",
    "            route = data.get(\"route\", \"other\")\n",
    "            confidence = float(data.get(\"confidence\", 0.6))\n",
    "        except Exception:\n",
    "            route = \"other\"\n",
    "            confidence = 0.5\n",
    "        return {\"route\": route, \"confidence\": confidence}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4a3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, TypedDict\n",
    "\n",
    "class PlanStep(TypedDict):\n",
    "    id: int\n",
    "    description: str\n",
    "    tool_family: str  # e.g. \"gsc\", \"dataforseo_keywords\", \"none\"\n",
    "\n",
    "class PlanOutput(TypedDict):\n",
    "    steps: List[PlanStep]\n",
    "\n",
    "PLANNER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a planning agent for an SEO + general assistant system.\n",
    "\n",
    "You receive:\n",
    "- the user query\n",
    "- a selected high-level route: \"seo\", \"other\", or \"mixed\"\n",
    "\n",
    "You must break the task into 1-5 concise steps.\n",
    "Each step should optionally mention a TOOL FAMILY, not a concrete function name.\n",
    "\n",
    "Examples of tool families:\n",
    "- \"gsc\"                       # Google Search Console tools\n",
    "- \"dataforseo_keywords\"       # keyword / SERP API tools\n",
    "- \"dataforseo_competitors\"    # competitor APIs\n",
    "- \"vector_cache\"              # internal semantic cache\n",
    "- \"none\"                      # pure reasoning / no tool\n",
    "\n",
    "Return STRICT JSON:\n",
    "{\"steps\": [{\"id\": 1, \"description\": \"...\", \"tool_family\": \"gsc\" }, ...]}\n",
    "\"\"\"\n",
    "\n",
    "class PlannerAgent:\n",
    "    def __init__(self, llm: LLMClient):\n",
    "        self.llm = llm\n",
    "\n",
    "    async def plan(self, user_query: str, route: str) -> PlanOutput:\n",
    "        user_content = f\"Route: {route}\\n\\nUser query:\\n{user_query}\"\n",
    "        resp = await self.llm.call(PLANNER_SYSTEM_PROMPT, user_content)\n",
    "        try:\n",
    "            data = json.loads(resp.content)\n",
    "        except Exception:\n",
    "            data = {\"steps\": [{\"id\": 1, \"description\": \"Answer directly.\", \"tool_family\": \"none\"}]}\n",
    "        return data  # type: ignore[return-value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25a3d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents/seo_executor_agent.py\n",
    "from typing import List, Dict, Any\n",
    "from langchain.tools import tool\n",
    "\n",
    "# ---------- Define your SEO tools here ----------\n",
    "\n",
    "@tool(\"gsc_get_overview\")\n",
    "def gsc_get_overview(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get high-level performance overview for a URL/domain from GSC.\n",
    "    IMPLEMENTATION: call your MCP GSC tool here.\n",
    "    \"\"\"\n",
    "    # TODO: wire up MCP client / DataForSEO / HTTP call\n",
    "    return {\"source\": \"GSC\", \"url\": url, \"clicks\": 123, \"impressions\": 456}\n",
    "\n",
    "@tool(\"dataforseo_keywords_ideas\")\n",
    "def dataforseo_keywords_ideas(query: str, location: str = \"India\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get keyword ideas from DataForSEO for a given query and location.\n",
    "    IMPLEMENTATION: call DataForSEO MCP tool here.\n",
    "    \"\"\"\n",
    "    # TODO: real implementation here\n",
    "    return {\n",
    "        \"source\": \"DataForSEO\",\n",
    "        \"query\": query,\n",
    "        \"location\": location,\n",
    "        \"keywords\": [\n",
    "            {\"keyword\": f\"{query} tools\", \"volume\": 2900},\n",
    "            {\"keyword\": f\"{query} pricing\", \"volume\": 1300},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "SEO_TOOLS = [gsc_get_overview, dataforseo_keywords_ideas]\n",
    "\n",
    "SEO_EXECUTOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are an SEO execution agent.\n",
    "\n",
    "- You receive the user query and a list of planned steps.\n",
    "- You can call ONLY the tools you see (GSC + DataForSEO).\n",
    "- For each step, decide whether to call a tool or just reason.\n",
    "- Prefer 1-3 tool calls, not more, unless necessary.\n",
    "- After tools are called, synthesize a structured RESULT OBJECT summarizing what you found.\n",
    "\n",
    "Return your final answer as JSON:\n",
    "{\n",
    "  \"tool_calls\": [...],   # brief description + tool name\n",
    "  \"insights\": [...],     # concise SEO insights\n",
    "  \"raw_data\": {...}      # aggregated raw tool outputs (optional)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class SEOExecutorAgent:\n",
    "    def __init__(self, llm: LLMClient):\n",
    "        self.llm = llm\n",
    "\n",
    "    async def execute(\n",
    "        self,\n",
    "        user_query: str,\n",
    "        plan_steps: List[Dict[str, Any]],\n",
    "    ) -> Any:\n",
    "        # Put the plan into the context as markdown / JSON\n",
    "        steps_str = \"\\n\".join(\n",
    "            [f\"- Step {s['id']}: {s['description']} [family={s['tool_family']}]\" for s in plan_steps]\n",
    "        )\n",
    "        content = f\"User query:\\n{user_query}\\n\\nPlanned steps:\\n{steps_str}\"\n",
    "\n",
    "        resp = await self.llm.call(\n",
    "            system_prompt=SEO_EXECUTOR_SYSTEM_PROMPT,\n",
    "            user_content=content,\n",
    "            tools=SEO_TOOLS,\n",
    "        )\n",
    "        return resp  # resp.content + resp.tool_calls etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5509e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a summarization and explanation agent.\n",
    "\n",
    "Input:\n",
    "- User's original query\n",
    "- A JSON blob with 'tool_calls', 'insights', and 'raw_data'\n",
    "\n",
    "Your job:\n",
    "- Write a clear, concise, user-facing answer.\n",
    "- Focus on ACTIONABLE SEO advice, not verbose theory.\n",
    "- Use bullet points where helpful.\n",
    "- Mention specific metrics/keywords only if present in the JSON.\n",
    "\n",
    "Do NOT show raw JSON back to the user.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizerAgent:\n",
    "    def __init__(self, llm: LLMClient):\n",
    "        self.llm = llm\n",
    "\n",
    "    async def summarize(self, user_query: str, execution_json: str) -> str:\n",
    "        user_content = f\"User query:\\n{user_query}\\n\\nExecution result JSON:\\n{execution_json}\"\n",
    "        resp = await self.llm.call(SUMMARIZER_SYSTEM_PROMPT, user_content)\n",
    "        return resp.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cd681f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator.py\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Any, Dict\n",
    "\n",
    "class Orchestrator:\n",
    "    def __init__(self):\n",
    "        llm_router = LLMClient(model=\"gpt-4.1\", temperature=0.0)\n",
    "        llm_planner = LLMClient(model=\"gpt-4.1\", temperature=0.2)\n",
    "        llm_seo = LLMClient(model=\"gpt-4.1\", temperature=0.2)\n",
    "        llm_summarizer = LLMClient(model=\"gpt-4.1\", temperature=0.4)\n",
    "\n",
    "        self.router = RouterAgent(llm_router)\n",
    "        self.planner = PlannerAgent(llm_planner)\n",
    "        self.seo_executor = SEOExecutorAgent(llm_seo)\n",
    "        self.summarizer = SummarizerAgent(llm_summarizer)\n",
    "\n",
    "    async def handle_query(self, user_query: str) -> str:\n",
    "        # 1) ROUTER\n",
    "        route_result = await self.router.route(user_query)\n",
    "        route = route_result[\"route\"]\n",
    "\n",
    "        # 2) PLANNER\n",
    "        plan_result = await self.planner.plan(user_query, route)\n",
    "        steps = plan_result[\"steps\"]\n",
    "\n",
    "        # 3) EXECUTION (branch by route)\n",
    "        if route in (\"seo\", \"mixed\"):\n",
    "            exec_resp = await self.seo_executor.execute(user_query, steps)\n",
    "            # exec_resp.content should be JSON string (per SEO_EXECUTOR_SYSTEM_PROMPT)\n",
    "            exec_json_str = exec_resp.content\n",
    "        else:\n",
    "            # For now: directly answer without tools (you can later add a generic executor)\n",
    "            exec_json_str = json.dumps(\n",
    "                {\n",
    "                    \"tool_calls\": [],\n",
    "                    \"insights\": [f\"Direct reasoning for query: {user_query}\"],\n",
    "                    \"raw_data\": {},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # 4) SUMMARIZER\n",
    "        final_answer = await self.summarizer.summarize(user_query, exec_json_str)\n",
    "        return final_answer\n",
    "\n",
    "\n",
    "async def main():\n",
    "    orch = Orchestrator()\n",
    "    user_query = \"Provide SEO analysis for domain strique.io\"\n",
    "    answer = await orch.handle_query(user_query)\n",
    "    print(\"\\n=== FINAL ANSWER ===\\n\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c50d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL ANSWER ===\n",
      "\n",
      "It looks like there is no SEO data available for strique.io at this time. Please ensure the domain is correct or try again later with more information. If you need actionable SEO advice, consider the following general steps:\n",
      "\n",
      "- Check if your site is indexed by Google using \"site:strique.io\" in Google search.\n",
      "- Ensure your website has unique title tags and meta descriptions for each page.\n",
      "- Optimize on-page content with relevant keywords.\n",
      "- Improve site speed and mobile responsiveness.\n",
      "- Build quality backlinks from reputable sources.\n",
      "- Set up Google Search Console and Google Analytics for ongoing monitoring.\n",
      "\n",
      "If you provide more specific data or metrics, I can give more tailored recommendations!\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ab25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "class SEOAgent:\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.name = \"Simple Query Agent\"\n",
    "        self.description = \n",
    "        self.model = llm\n",
    "        self._tools = None\n",
    "        self._agent = None\n",
    "        self._tool_warning: Optional[str] = None\n",
    "        self._mcp_client: Optional[MultiServerMCPClient] = None\n",
    "\n",
    "    def _build_gsc_server_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build configuration for the local GSC MCP server.\"\"\"\n",
    "        current_dir = Path(__file__).parent\n",
    "        project_root = current_dir.parent.parent\n",
    "        gsc_server_path = project_root / \"src\" / \"tools\" / \"gsc_server.py\"\n",
    "\n",
    "        python_interpreter = sys.executable\n",
    "\n",
    "        return {\n",
    "            \"command\": python_interpreter,\n",
    "            \"args\": [str(gsc_server_path)],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"env\": {\n",
    "                \"GSC_CREDENTIALS\": os.getenv(\"GSC_CREDENTIALS\", \"\"),\n",
    "                \"GSC_SKIP_OAUTH\": os.getenv(\"GSC_SKIP_OAUTH\", \"true\"),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _build_dataforseo_config(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Build configuration for the optional DataForSEO MCP server.\"\"\"\n",
    "        enable_remote = os.getenv(\"ENABLE_DATAFORSEO_MCP\", \"true\").lower() in (\"1\", \"true\", \"yes\")\n",
    "        if not enable_remote:\n",
    "            return None\n",
    "\n",
    "        default_url = \"https://dataforseo-mcp-worker.hitesh-solanki.workers.dev/mcp\"\n",
    "        dataforseo_url = os.getenv(\"DATAFORSEO_MCP_URL\", default_url).strip()\n",
    "        if not dataforseo_url:\n",
    "            return None\n",
    "\n",
    "        config: Dict[str, Any] = {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": dataforseo_url,\n",
    "        }\n",
    "\n",
    "        timeout = os.getenv(\"DATAFORSEO_MCP_TIMEOUT\")\n",
    "        if timeout:\n",
    "            try:\n",
    "                config[\"timeout\"] = float(timeout)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        auth_header = os.getenv(\"DATAFORSEO_MCP_AUTH_HEADER\")\n",
    "        if auth_header:\n",
    "            config[\"headers\"] = {\"Authorization\": auth_header.strip()}\n",
    "\n",
    "        return config\n",
    "\n",
    "    def _build_server_config(self, include_dataforseo: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Build the MCP server configuration dictionary.\"\"\"\n",
    "        servers: Dict[str, Any] = {\n",
    "            \"gscServer\": self._build_gsc_server_config()\n",
    "        }\n",
    "\n",
    "        if include_dataforseo:\n",
    "            dataforseo_config = self._build_dataforseo_config()\n",
    "            if dataforseo_config:\n",
    "                servers[\"dataforseo\"] = dataforseo_config\n",
    "\n",
    "        return servers\n",
    "\n",
    "    def get_mcp_client(self, include_dataforseo: bool = True):\n",
    "        \"\"\"Initialize and return the MCP client with configured servers.\"\"\"\n",
    "        config = self._build_server_config(include_dataforseo=include_dataforseo)\n",
    "        return MultiServerMCPClient(config)\n",
    "    \n",
    "    async def get_tools(self):\n",
    "        \"\"\"Get tools from MCP clients. Caches tools for reuse.\"\"\"\n",
    "        if self._tools is None:\n",
    "            self._tool_warning = None\n",
    "\n",
    "            # Try loading with both servers first\n",
    "            client = self.get_mcp_client(include_dataforseo=True)\n",
    "            try:\n",
    "                self._mcp_client = client\n",
    "                self._tools = await client.get_tools()\n",
    "                return self._tools\n",
    "            except Exception as exc:\n",
    "                # If DataForSEO is enabled and fails, fall back to GSC-only\n",
    "                server_config = self._build_server_config(include_dataforseo=True)\n",
    "                if \"dataforseo\" in server_config:\n",
    "                    fallback_client = self.get_mcp_client(include_dataforseo=False)\n",
    "                    try:\n",
    "                        details = str(exc)\n",
    "                        if len(details) > 500:\n",
    "                            details = details[:500] + \"... (truncated)\"\n",
    "                        self._tool_warning = (\n",
    "                            \"⚠️ DataForSEO MCP server could not be reached. \"\n",
    "                            \"Continuing with Google Search Console tools only.\\n\"\n",
    "                            f\"Details: {details}\"\n",
    "                        )\n",
    "                        self._mcp_client = fallback_client\n",
    "                        self._tools = await fallback_client.get_tools()\n",
    "                        return self._tools\n",
    "                    except Exception as fallback_exc:\n",
    "                        raise fallback_exc from exc\n",
    "                # No fallback available, re-raise\n",
    "                raise\n",
    "        return self._tools\n",
    "\n",
    "    def get_tool_warning(self) -> Optional[str]:\n",
    "        \"\"\"Return any warning generated while loading tools.\"\"\"\n",
    "        return self._tool_warning\n",
    "\n",
    "    def update_system_prompt(self, new_prompt: str):\n",
    "        \"\"\"Update the system prompt and invalidate the cached agent.\"\"\"\n",
    "        self.description = new_prompt\n",
    "        # Invalidate the cached agent so it will be recreated with the new prompt\n",
    "        self._agent = None\n",
    "\n",
    "    async def get_agent(self):\n",
    "        \"\"\"Get or create the agent with tools and error handling middleware.\"\"\"\n",
    "        if self._agent is None:\n",
    "            # Get the tools\n",
    "            tools = await self.get_tools()\n",
    "            \n",
    "            # Create the agent with error handling middleware\n",
    "            self._agent = create_agent(\n",
    "                model=self.model,\n",
    "                tools=tools,\n",
    "                system_prompt=self.description,\n",
    "                middleware=[handle_tool_errors]\n",
    "            )\n",
    "        \n",
    "        return self._agent\n",
    "        \n",
    "    async def run(self, messages):\n",
    "        \"\"\"Run the agent with the given messages.\"\"\"\n",
    "        # Get the agent\n",
    "        agent = await self.get_agent()\n",
    "        \n",
    "        # Run the agent\n",
    "        result = await agent.ainvoke({\n",
    "            \"messages\": messages\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def stream(self, messages):\n",
    "        \"\"\"Stream agent responses as they are generated.\"\"\"\n",
    "        # Get the agent\n",
    "        agent = await self.get_agent()\n",
    "        \n",
    "        # Stream the agent's response\n",
    "        async for chunk in agent.astream(\n",
    "            {\"messages\": messages},\n",
    "            stream_mode=\"values\"\n",
    "        ):\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55744a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.tools.seo_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseo_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeoTools\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseo_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SEOAgent\n\u001b[32m      5\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4.1\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.tools.seo_tools'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c15ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
